{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mailbox import mbox\n",
    "import pandas as pd\n",
    "\n",
    "def store_content(message, body=None):\n",
    "    if not body:\n",
    "        body = message.get_payload(decode=True)\n",
    "    if len(message):\n",
    "        contents = {\n",
    "            \"subject\": message[\"subject\"] or \"\",\n",
    "            \"body\": body,\n",
    "            \"from\": message[\"from\"],\n",
    "            \"to\": message[\"to\"],\n",
    "            \"date\": message[\"date\"],\n",
    "            \"labels\": message[\"X-Gmail-Labels\"],\n",
    "            \"epilogue\": message.epilogue,\n",
    "        }\n",
    "        return df.append(contents, ignore_index=True)\n",
    "\n",
    "# Create empty DataFrame with relevant columns\n",
    "df = pd.DataFrame(\n",
    "    columns=(\"subject\", \"body\", \"from\", \"to\", \"date\", \"labels\", \"epilogue\")\n",
    ")\n",
    "\n",
    "# Import downloaded mbox file\n",
    "box = mbox(\"Takeout/Mail/AllmailIncludingSpamandTrash.mbox\")\n",
    "\n",
    "fails = []\n",
    "for message in box:\n",
    "    try:\n",
    "        if message.get_content_type() == 'text/plain':\n",
    "            df = store_content(message)\n",
    "        elif message.is_multipart():\n",
    "            # Grab any plaintext from multipart messages\n",
    "            for part in message.get_payload():\n",
    "                if part.get_content_type() == 'text/plain':\n",
    "                    df = store_content(message, part.get_payload(decode=True))\n",
    "                    break\n",
    "    except:\n",
    "        fails.append(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('to', 4041), ('off', 3303), ('for', 3283), ('on', 3053), ('the', 3046), ('new', 2983), ('your', 2569), ('&', 2560), ('and', 2422), ('-', 2370)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "subject_word_bag = df.subject.apply(lambda t: str(t).lower() + \" \").sum()\n",
    "\n",
    "print(Counter(subject_word_bag.split()).most_common()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('new', 2983), ('free', 1142), ('save', 1001), ('|', 867), ('sale', 834), ('today', 740), ('20%', 732), ('online', 712), ('extra', 644), ('commented', 639)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = [str(word) for word in stopwords.words('english')] + ['re:', 'fwd:', '_', '&', '-', '+']\n",
    "subject_words = [word for word in subject_word_bag.split() if str(word).lower() not in stops]\n",
    "print(Counter(subject_words).most_common()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('new', 'jobs'), 0.0032635591520096302)\n",
      "(('free', 'shipping'), 0.002808800909516485)\n",
      "(('free', 'shipping.'), 0.002287166454891995)\n",
      "(('bar', 'room'), 0.0022136026215475156)\n",
      "(('[the', 'bar'), 0.002193539757908112)\n",
      "(('room', 'heroes]'), 0.002193539757908112)\n",
      "(('stores', 'online'), 0.002066474954858557)\n",
      "(('15', 'new'), 0.0016050290911522771)\n",
      "(('extra', '30%'), 0.0014779642881027218)\n",
      "(('photo', 'you.'), 0.0014177756971845115)\n"
     ]
    }
   ],
   "source": [
    "from nltk import collocations\n",
    "\n",
    "# Analyzing most frequently paired words in email subjects\n",
    "bigram_measures = collocations.BigramAssocMeasures()\n",
    "bigram_finder = collocations.BigramCollocationFinder.from_words(subject_words)\n",
    "\n",
    "# Filter to top 20 results; otherwise this will take a LONG time\n",
    "bigram_finder.apply_freq_filter(20)\n",
    "for bigram in bigram_finder.score_ngrams(bigram_measures.raw_freq)[:10]:\n",
    "    print(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('[the', 'bar', 'room'), 0.002193539757908112)\n",
      "(('bar', 'room', 'heroes]'), 0.002193539757908112)\n",
      "(('15', 'new', 'jobs'), 0.0015916538487260083)\n",
      "(('developer', 'wilkes', 'barre,'), 0.0012171470607904768)\n",
      "(('wilkes', 'barre,', 'pa'), 0.0012171470607904768)\n",
      "(('jobs', 'developer', 'wilkes'), 0.0012104594395773423)\n",
      "(('new', 'jobs', 'developer'), 0.0012104594395773423)\n",
      "(('design', 'free', 'shipping.'), 0.0011703337122985354)\n",
      "(('everyday', 'design', 'free'), 0.0011703337122985354)\n",
      "(('commented', 'dwight', \"addington's\"), 0.001150270848659132)\n"
     ]
    }
   ],
   "source": [
    "from nltk import collocations\n",
    "\n",
    "# Analyzing most frequent groupings of three words in email subjects\n",
    "trigram_measures = collocations.TrigramAssocMeasures()\n",
    "trigram_finder = collocations.TrigramCollocationFinder.from_words(subject_words)\n",
    "\n",
    "# Filter to top 20 results; otherwise this will take a LONG time\n",
    "trigram_finder.apply_freq_filter(20)\n",
    "for trigram in trigram_finder.score_ngrams(trigram_measures.raw_freq)[:10]:\n",
    "    print(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('refurb', 'maestro')\n",
      "('avengers', 'avengers')\n",
      "('maestro', 'grinder')\n",
      "('foundations', 'frontiers]')\n",
      "('connections,', 'experience,')\n",
      "('tanya', 'pi')\n",
      "('[laff:', 'linear')\n",
      "('biography', 'memoir')\n",
      "('categories', 'biography')\n",
      "('leila', \"brewster's\")\n"
     ]
    }
   ],
   "source": [
    "from nltk import collocations\n",
    "\n",
    "# Analyzing pairs of words with pointwise mutual information,\n",
    "# which gives us a word that is most often used with another word.\n",
    "bigram_measures = collocations.BigramAssocMeasures()\n",
    "bigram_finder = collocations.BigramCollocationFinder.from_words(subject_words)\n",
    "\n",
    "# Filter to top 20 results; otherwise this will take a LONG time\n",
    "bigram_finder.apply_freq_filter(20)\n",
    "for bigram in bigram_finder.nbest(bigram_measures.pmi, 10):\n",
    "    print(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('refurb', 'maestro', 'grinder')\n",
      "('avengers', 'avengers', 'avengers')\n",
      "('categories', 'biography', 'memoir')\n",
      "('problem', 'refurb', 'maestro')\n",
      "('algebra', 'foundations', 'frontiers]')\n",
      "('maestro', 'grinder', 'problem')\n",
      "('[laff:', 'linear', 'algebra')\n",
      "('grinder', 'problem', 'refurb')\n",
      "('\"epson', 'connect', 'scan')\n",
      "('connect', 'scan', 'cloud\"')\n"
     ]
    }
   ],
   "source": [
    "from nltk import collocations\n",
    "\n",
    "# Analyzes groupings of three words with pointwise\n",
    "# mutual information\n",
    "trigram_measures = collocations.TrigramAssocMeasures()\n",
    "trigram_finder = collocations.TrigramCollocationFinder.from_words(subject_words)\n",
    "\n",
    "# Filter to top 20 results; otherwise this will take a LONG time\n",
    "trigram_finder.apply_freq_filter(20)\n",
    "for trigram in trigram_finder.nbest(trigram_measures.pmi, 10):\n",
    "    print(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 subject     feels\n",
      "0                         Omayeli Arenyeka posted a note  0.000000\n",
      "1          See Danny's connections, experience, and more  0.500000\n",
      "2      =?utf-8?Q?Free=20for=2048=20Hours=3A=20New=20R...  0.000000\n",
      "3      Suggestions based on stephanie yee, + POOL and...  0.000000\n",
      "4      =?utf-8?Q?The=20best=20=28and=20worst=29=20nut...  0.000000\n",
      "5                Netted | This Email May Induce Drooling  0.000000\n",
      "6           See Emma's connections, experience, and more  0.500000\n",
      "7      8 Surprisingly Easy Ways You Can Wow the Perso...  0.266667\n",
      "8      [NYEdTech] You are Invited - EdTech Event @ CC...  0.000000\n",
      "9      =?utf-8?q?All_New_Fonts=2C_Graphics=2C_Templat...  0.000000\n",
      "10     =?utf-8?q?All_New_Fonts=2C_Graphics=2C_Templat...  0.000000\n",
      "11                        Docker Weekly: July 29th, 2015  0.000000\n",
      "12     =?utf-8?Q?Joshua=20=2D=20want=20a=20better=20w...  0.000000\n",
      "13     The New Denim Style Guide - Find Your Perfect Fit  0.512121\n",
      "14     Pillows starting at $39.99 + Extra 30% off Mar...  0.000000\n",
      "15         [Ends Soon!] Save $12 on Sublime Productivity  0.000000\n",
      "16                         Jen Craig is expecting a baby  0.000000\n",
      "17     =?utf-8?Q?Clone=20AirBnb=20with=20Ruby=20on=20...  0.000000\n",
      "18         Danny Mendoza wants to be friends on Facebook  0.200000\n",
      "19                  6 Ways to Make the Best Tomato Salad  1.000000\n",
      "20               Meet your expert technology instructors  0.000000\n",
      "21            Beaches, Bmw and other interests to follow -0.125000\n",
      "22     NEW! Pasta in Minutes + Extra 30% Off Summer Sale  0.085227\n",
      "23     breaking: House passes the ultimate Monsanto P...  0.000000\n",
      "24     [Talk | New York Civic Technology News & Discu...  0.136364\n",
      "25     =?UTF-8?B?U29vbiB0aGUgaG90bmVzcyB3aWxsIGJlIGdv...  0.000000\n",
      "26     3 easy breakfast recipes you can make before work  0.433333\n",
      "27     Take an EXTRA 40% off Clearance! 6 hours only ...  0.104167\n",
      "28     Register by August 3 to secure the hottest dev...  0.400000\n",
      "29                                           last chance  0.000000\n",
      "...                                                  ...       ...\n",
      "27522  Starts TODAY! 20% Off Any 1 Item + More Labor ...  0.500000\n",
      "27523  Fall Prep + Labor Day Sale Continues With 25% ... -0.100000\n",
      "27524       USSOCOM Awards Polaris $83M Defense Contract  0.000000\n",
      "27525           Re: [JAWS] add interactive tagging (#90) -0.750000\n",
      "27526               [JAWS] add interactive tagging (#90)  0.000000\n",
      "27527  Joshua, join Ivette Addington, Wesley Addingto... -0.125000\n",
      "27528                A Very Exclusive Gift with Purchase  0.200000\n",
      "27529  2 new jobs for Junior Python Developer in New ...  0.136364\n",
      "27530                                Something Different  0.000000\n",
      "27531                   Geisha Release for Sept 14 Roast  0.000000\n",
      "27532  Win 2-years of Premium and an Amazon Fire TV S...  1.000000\n",
      "27533  =?UTF-8?Q?=e2=9C=94?= Thanks for Being a Regis...  0.312500\n",
      "27534                      [JAWS] Any ETA on v1.0? (#91)  0.000000\n",
      "27535                  Re: [JAWS] Any ETA on v1.0? (#91) -0.750000\n",
      "27536                  Re: [JAWS] Any ETA on v1.0? (#91) -0.750000\n",
      "27537                  Re: [JAWS] Any ETA on v1.0? (#91) -0.750000\n",
      "27538                  Re: [JAWS] Any ETA on v1.0? (#91) -0.750000\n",
      "27539  What are some of the easy things that anyone c...  0.433333\n",
      "27540  15 Recipes You Can Stuff in a Pita | 9 Blogs f... -0.233333\n",
      "27541  LABOR DAY SAVINGS: 20% Off Tabletop + Addition...  0.500000\n",
      "27542  =?UTF-8?Q?=E2=80=9CA_Breakup_Letter_From_The_I...  0.000000\n",
      "27543  Your message to NYC-rb-list@meetup.com has not...  0.000000\n",
      "27544  Today Only - Take $50 Off - Free Shipping On O...  0.300000\n",
      "27545          Stephanie Matter-Schock added a new photo  0.136364\n",
      "27546                  15 Best-Ever Grilled Fish Recipes  0.000000\n",
      "27547                       Joshua, 10 new jobs for you.  0.136364\n",
      "27548  =?utf-8?B?4p25IEZBVk9SSVRFUyEgVXAgdG8gJDEwMCBP...  0.000000\n",
      "27549  =?utf-8?B?WW91ciBBdWd1c3QgaG9yb3Njb3Bl4oCUQklH...  0.000000\n",
      "27550                     Sarah Keener added a new photo  0.136364\n",
      "27551                 Charlotte Wiley updated her status  0.000000\n",
      "\n",
      "[27552 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# scores the sentiment of email subjects from 1.0 for positive\n",
    "# to -1.0 for negative.\n",
    "df['feels'] = df.subject.apply(\n",
    "    lambda s: TextBlob(str(s)).sentiment.polarity\n",
    ")\n",
    "\n",
    "# Output a few subject lines with the calculated sentiment scores\n",
    "print(df[['subject', 'feels']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
